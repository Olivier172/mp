{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8736c055",
   "metadata": {},
   "source": [
    "# Validation SSL model\n",
    "\n",
    "In this notebook our goal is to test how good our SSL pretrained weights are. \n",
    "- We will query images from different classes and compare embeddings. This will give us better insights for the intraclass/interclass variability.\n",
    "    - Intraclass variance: variance within one class (The intraclass variance measures the differences between the individual embeddings within each class.)\n",
    "    - Interclass variance: variance between different classes (The interclass variance measures the differences between the means of each class)\n",
    "- Note: you need to run this notebook with a kernel in your venv to use vissl libs: https://janakiev.com/blog/jupyter-virtual-envs/#add-virtual-environment-to-jupyter-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6c712",
   "metadata": {},
   "source": [
    "## Imports\n",
    "- matplotlib for visualisation\n",
    "- torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ebd1b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df5f44e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ebec0",
   "metadata": {},
   "source": [
    "## Reading in pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1105b3d",
   "metadata": {},
   "source": [
    "### Option 1: Imagenet pretrained\n",
    "- Load the best imgnet pretrained weights, docs: https://pytorch.org/vision/stable/models.html\n",
    "- This is currently ResNet50_Weights.IMAGENET1K_V2 with an accuracy of 80.858%\n",
    "- weights are saved in /home/olivier/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "523d9938",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.models' has no attribute 'ResNet50_Weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#imgnet weights\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mresnet50(weights\u001b[38;5;241m=\u001b[39m\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResNet50_Weights\u001b[49m\u001b[38;5;241m.\u001b[39mDEFAULT)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#torch.save(model.state_dict(),\"resnet50_imgnet.pth\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet50_imgnet.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.models' has no attribute 'ResNet50_Weights'"
     ]
    }
   ],
   "source": [
    "#imgnet weights\n",
    "model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "#torch.save(model.state_dict(),\"resnet50_imgnet.pth\")\n",
    "weights = torch.load(\"resnet50_imgnet.pth\")\n",
    "#print(weights.keys())\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4716f403",
   "metadata": {},
   "source": [
    "### Option 2: SSL pretrained\n",
    "Load weights from checkpoint according to vissl tutorial:\n",
    "https://github.com/facebookresearch/vissl/blob/v0.1.6/tutorials/Using_a_pretrained_model_for_inference_V0_1_6.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3282ada2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.common.file_io:** fvcore version of PathManager will be deprecated soon. **\n",
      "** Please migrate to the version in iopath repo. **\n",
      "https://github.com/facebookresearch/iopath \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from vissl.utils.hydra_config import AttrDict\n",
    "from vissl.utils.hydra_config import compose_hydra_configuration, convert_to_attrdict\n",
    "\n",
    "# Checkpoint config is located at vissl/configs/config/validation.\n",
    "# weights are located at \n",
    "# All other options override the train_config.yaml config.\n",
    "\n",
    "cfg = [\n",
    "  'config=validation/rotnet_full/train_config.yaml',\n",
    "  'config.MODEL.WEIGHTS_INIT.PARAMS_FILE=/home/olivier/Documents/master/mp/checkpoints/sku110k/rotnet_full/model_final_checkpoint_phase104.torch', # Specify path for the model weights.\n",
    "  'config.MODEL.FEATURE_EVAL_SETTINGS.EVAL_MODE_ON=True', # Turn on model evaluation mode.\n",
    "  'config.MODEL.FEATURE_EVAL_SETTINGS.FREEZE_TRUNK_ONLY=True', # Freeze trunk. \n",
    "  'config.MODEL.FEATURE_EVAL_SETTINGS.EXTRACT_TRUNK_FEATURES_ONLY=True', # Extract the trunk features, as opposed to the HEAD.\n",
    "  'config.MODEL.FEATURE_EVAL_SETTINGS.SHOULD_FLATTEN_FEATS=False', # Do not flatten features.\n",
    "  'config.MODEL.FEATURE_EVAL_SETTINGS.LINEAR_EVAL_FEAT_POOL_OPS_MAP=[[\"res5avg\", [\"Identity\", []]]]' # Extract only the res5avg features.\n",
    "]\n",
    "\n",
    "# Compose the hydra configuration.\n",
    "cfg = compose_hydra_configuration(cfg)\n",
    "# Convert to AttrDict. This method will also infer certain config options\n",
    "# and validate the config is valid.\n",
    "_, cfg = convert_to_attrdict(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6e3f0",
   "metadata": {},
   "source": [
    "Now let's build the model with the exact training configs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce794485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vissl.models import build_model\n",
    "\n",
    "model = build_model(cfg.MODEL, cfg.OPTIMIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a322b7",
   "metadata": {},
   "source": [
    "#### Loading the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1b5622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights have loaded\n"
     ]
    }
   ],
   "source": [
    "from classy_vision.generic.util import load_checkpoint\n",
    "from vissl.utils.checkpoint import init_model_from_consolidated_weights\n",
    "\n",
    "# Load the checkpoint weights.\n",
    "weights = load_checkpoint(checkpoint_path=cfg.MODEL.WEIGHTS_INIT.PARAMS_FILE)\n",
    "\n",
    "\n",
    "# Initializei the model with the simclr model weights.\n",
    "init_model_from_consolidated_weights(\n",
    "    config=cfg,\n",
    "    model=model,\n",
    "    state_dict=weights,\n",
    "    state_dict_key_name=\"classy_state_dict\",\n",
    "    skip_layers=[],  # Use this if you do not want to load all layers\n",
    ")\n",
    "\n",
    "print(\"Weights have loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9120f9",
   "metadata": {},
   "source": [
    "#### Extra info\n",
    "- VISSL uses the ResNeXT50 class, which is their custom wrapper class\n",
    "    - ResNeXT50 wrapper class is defined at https://github.com/facebookresearch/vissl/blob/04788de934b39278326331f7a4396e03e85f6e55/vissl/models/trunks/resnext.py\n",
    "    - ResNet base class https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py for interface of the __init__ method.\n",
    "    - the model of this wrapper class is a torchvision.models.ResNet() which we will reconstruct here based on the YAML config parameters.\n",
    "- checkpoints from pretraining are stored on /home/olivier/Documents/master/mp/checkpoints/sku110k/\n",
    "    - checkpoints have phase numbers: in VISSL, if the workflow involves training and testing both, the number of phases = train phases + test epochs. So if we alternate train and test, the phase number is: 0 (train), 1 (test), 2 (train), 3 (test)... and train_phase_idx is always: 0 (corresponds to phase0), 1 (correponds to phase 2)\n",
    "    - The weights are stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7238a7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vissl checkpoint\n",
      "Checkpoint contains:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olivier/Documents/master/mp/.venv/lib/python3.8/site-packages/torch/cuda/__init__.py:104: UserWarning: \n",
      "NVIDIA GeForce RTX 4070 Laptop GPU with CUDA capability sm_89 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 4070 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>phase_idx</th>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_num</th>\n",
       "      <td>4007807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_phase_idx</th>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration</th>\n",
       "      <td>3948315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>consolidated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Value\n",
       "phase_idx                 125\n",
       "iteration_num         4007807\n",
       "train_phase_idx           104\n",
       "iteration             3948315\n",
       "type             consolidated"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint also contains elements loss and classy_state_dict\n",
      "odict_keys(['_feature_blocks.conv1.weight', '_feature_blocks.bn1.weight', '_feature_blocks.bn1.bias', '_feature_blocks.bn1.running_mean', '_feature_blocks.bn1.running_var', '_feature_blocks.bn1.num_batches_tracked', '_feature_blocks.layer1.0.conv1.weight', '_feature_blocks.layer1.0.bn1.weight', '_feature_blocks.layer1.0.bn1.bias', '_feature_blocks.layer1.0.bn1.running_mean', '_feature_blocks.layer1.0.bn1.running_var', '_feature_blocks.layer1.0.bn1.num_batches_tracked', '_feature_blocks.layer1.0.conv2.weight', '_feature_blocks.layer1.0.bn2.weight', '_feature_blocks.layer1.0.bn2.bias', '_feature_blocks.layer1.0.bn2.running_mean', '_feature_blocks.layer1.0.bn2.running_var', '_feature_blocks.layer1.0.bn2.num_batches_tracked', '_feature_blocks.layer1.0.conv3.weight', '_feature_blocks.layer1.0.bn3.weight', '_feature_blocks.layer1.0.bn3.bias', '_feature_blocks.layer1.0.bn3.running_mean', '_feature_blocks.layer1.0.bn3.running_var', '_feature_blocks.layer1.0.bn3.num_batches_tracked', '_feature_blocks.layer1.0.downsample.0.weight', '_feature_blocks.layer1.0.downsample.1.weight', '_feature_blocks.layer1.0.downsample.1.bias', '_feature_blocks.layer1.0.downsample.1.running_mean', '_feature_blocks.layer1.0.downsample.1.running_var', '_feature_blocks.layer1.0.downsample.1.num_batches_tracked', '_feature_blocks.layer1.1.conv1.weight', '_feature_blocks.layer1.1.bn1.weight', '_feature_blocks.layer1.1.bn1.bias', '_feature_blocks.layer1.1.bn1.running_mean', '_feature_blocks.layer1.1.bn1.running_var', '_feature_blocks.layer1.1.bn1.num_batches_tracked', '_feature_blocks.layer1.1.conv2.weight', '_feature_blocks.layer1.1.bn2.weight', '_feature_blocks.layer1.1.bn2.bias', '_feature_blocks.layer1.1.bn2.running_mean', '_feature_blocks.layer1.1.bn2.running_var', '_feature_blocks.layer1.1.bn2.num_batches_tracked', '_feature_blocks.layer1.1.conv3.weight', '_feature_blocks.layer1.1.bn3.weight', '_feature_blocks.layer1.1.bn3.bias', '_feature_blocks.layer1.1.bn3.running_mean', '_feature_blocks.layer1.1.bn3.running_var', '_feature_blocks.layer1.1.bn3.num_batches_tracked', '_feature_blocks.layer1.2.conv1.weight', '_feature_blocks.layer1.2.bn1.weight', '_feature_blocks.layer1.2.bn1.bias', '_feature_blocks.layer1.2.bn1.running_mean', '_feature_blocks.layer1.2.bn1.running_var', '_feature_blocks.layer1.2.bn1.num_batches_tracked', '_feature_blocks.layer1.2.conv2.weight', '_feature_blocks.layer1.2.bn2.weight', '_feature_blocks.layer1.2.bn2.bias', '_feature_blocks.layer1.2.bn2.running_mean', '_feature_blocks.layer1.2.bn2.running_var', '_feature_blocks.layer1.2.bn2.num_batches_tracked', '_feature_blocks.layer1.2.conv3.weight', '_feature_blocks.layer1.2.bn3.weight', '_feature_blocks.layer1.2.bn3.bias', '_feature_blocks.layer1.2.bn3.running_mean', '_feature_blocks.layer1.2.bn3.running_var', '_feature_blocks.layer1.2.bn3.num_batches_tracked', '_feature_blocks.layer2.0.conv1.weight', '_feature_blocks.layer2.0.bn1.weight', '_feature_blocks.layer2.0.bn1.bias', '_feature_blocks.layer2.0.bn1.running_mean', '_feature_blocks.layer2.0.bn1.running_var', '_feature_blocks.layer2.0.bn1.num_batches_tracked', '_feature_blocks.layer2.0.conv2.weight', '_feature_blocks.layer2.0.bn2.weight', '_feature_blocks.layer2.0.bn2.bias', '_feature_blocks.layer2.0.bn2.running_mean', '_feature_blocks.layer2.0.bn2.running_var', '_feature_blocks.layer2.0.bn2.num_batches_tracked', '_feature_blocks.layer2.0.conv3.weight', '_feature_blocks.layer2.0.bn3.weight', '_feature_blocks.layer2.0.bn3.bias', '_feature_blocks.layer2.0.bn3.running_mean', '_feature_blocks.layer2.0.bn3.running_var', '_feature_blocks.layer2.0.bn3.num_batches_tracked', '_feature_blocks.layer2.0.downsample.0.weight', '_feature_blocks.layer2.0.downsample.1.weight', '_feature_blocks.layer2.0.downsample.1.bias', '_feature_blocks.layer2.0.downsample.1.running_mean', '_feature_blocks.layer2.0.downsample.1.running_var', '_feature_blocks.layer2.0.downsample.1.num_batches_tracked', '_feature_blocks.layer2.1.conv1.weight', '_feature_blocks.layer2.1.bn1.weight', '_feature_blocks.layer2.1.bn1.bias', '_feature_blocks.layer2.1.bn1.running_mean', '_feature_blocks.layer2.1.bn1.running_var', '_feature_blocks.layer2.1.bn1.num_batches_tracked', '_feature_blocks.layer2.1.conv2.weight', '_feature_blocks.layer2.1.bn2.weight', '_feature_blocks.layer2.1.bn2.bias', '_feature_blocks.layer2.1.bn2.running_mean', '_feature_blocks.layer2.1.bn2.running_var', '_feature_blocks.layer2.1.bn2.num_batches_tracked', '_feature_blocks.layer2.1.conv3.weight', '_feature_blocks.layer2.1.bn3.weight', '_feature_blocks.layer2.1.bn3.bias', '_feature_blocks.layer2.1.bn3.running_mean', '_feature_blocks.layer2.1.bn3.running_var', '_feature_blocks.layer2.1.bn3.num_batches_tracked', '_feature_blocks.layer2.2.conv1.weight', '_feature_blocks.layer2.2.bn1.weight', '_feature_blocks.layer2.2.bn1.bias', '_feature_blocks.layer2.2.bn1.running_mean', '_feature_blocks.layer2.2.bn1.running_var', '_feature_blocks.layer2.2.bn1.num_batches_tracked', '_feature_blocks.layer2.2.conv2.weight', '_feature_blocks.layer2.2.bn2.weight', '_feature_blocks.layer2.2.bn2.bias', '_feature_blocks.layer2.2.bn2.running_mean', '_feature_blocks.layer2.2.bn2.running_var', '_feature_blocks.layer2.2.bn2.num_batches_tracked', '_feature_blocks.layer2.2.conv3.weight', '_feature_blocks.layer2.2.bn3.weight', '_feature_blocks.layer2.2.bn3.bias', '_feature_blocks.layer2.2.bn3.running_mean', '_feature_blocks.layer2.2.bn3.running_var', '_feature_blocks.layer2.2.bn3.num_batches_tracked', '_feature_blocks.layer2.3.conv1.weight', '_feature_blocks.layer2.3.bn1.weight', '_feature_blocks.layer2.3.bn1.bias', '_feature_blocks.layer2.3.bn1.running_mean', '_feature_blocks.layer2.3.bn1.running_var', '_feature_blocks.layer2.3.bn1.num_batches_tracked', '_feature_blocks.layer2.3.conv2.weight', '_feature_blocks.layer2.3.bn2.weight', '_feature_blocks.layer2.3.bn2.bias', '_feature_blocks.layer2.3.bn2.running_mean', '_feature_blocks.layer2.3.bn2.running_var', '_feature_blocks.layer2.3.bn2.num_batches_tracked', '_feature_blocks.layer2.3.conv3.weight', '_feature_blocks.layer2.3.bn3.weight', '_feature_blocks.layer2.3.bn3.bias', '_feature_blocks.layer2.3.bn3.running_mean', '_feature_blocks.layer2.3.bn3.running_var', '_feature_blocks.layer2.3.bn3.num_batches_tracked', '_feature_blocks.layer3.0.conv1.weight', '_feature_blocks.layer3.0.bn1.weight', '_feature_blocks.layer3.0.bn1.bias', '_feature_blocks.layer3.0.bn1.running_mean', '_feature_blocks.layer3.0.bn1.running_var', '_feature_blocks.layer3.0.bn1.num_batches_tracked', '_feature_blocks.layer3.0.conv2.weight', '_feature_blocks.layer3.0.bn2.weight', '_feature_blocks.layer3.0.bn2.bias', '_feature_blocks.layer3.0.bn2.running_mean', '_feature_blocks.layer3.0.bn2.running_var', '_feature_blocks.layer3.0.bn2.num_batches_tracked', '_feature_blocks.layer3.0.conv3.weight', '_feature_blocks.layer3.0.bn3.weight', '_feature_blocks.layer3.0.bn3.bias', '_feature_blocks.layer3.0.bn3.running_mean', '_feature_blocks.layer3.0.bn3.running_var', '_feature_blocks.layer3.0.bn3.num_batches_tracked', '_feature_blocks.layer3.0.downsample.0.weight', '_feature_blocks.layer3.0.downsample.1.weight', '_feature_blocks.layer3.0.downsample.1.bias', '_feature_blocks.layer3.0.downsample.1.running_mean', '_feature_blocks.layer3.0.downsample.1.running_var', '_feature_blocks.layer3.0.downsample.1.num_batches_tracked', '_feature_blocks.layer3.1.conv1.weight', '_feature_blocks.layer3.1.bn1.weight', '_feature_blocks.layer3.1.bn1.bias', '_feature_blocks.layer3.1.bn1.running_mean', '_feature_blocks.layer3.1.bn1.running_var', '_feature_blocks.layer3.1.bn1.num_batches_tracked', '_feature_blocks.layer3.1.conv2.weight', '_feature_blocks.layer3.1.bn2.weight', '_feature_blocks.layer3.1.bn2.bias', '_feature_blocks.layer3.1.bn2.running_mean', '_feature_blocks.layer3.1.bn2.running_var', '_feature_blocks.layer3.1.bn2.num_batches_tracked', '_feature_blocks.layer3.1.conv3.weight', '_feature_blocks.layer3.1.bn3.weight', '_feature_blocks.layer3.1.bn3.bias', '_feature_blocks.layer3.1.bn3.running_mean', '_feature_blocks.layer3.1.bn3.running_var', '_feature_blocks.layer3.1.bn3.num_batches_tracked', '_feature_blocks.layer3.2.conv1.weight', '_feature_blocks.layer3.2.bn1.weight', '_feature_blocks.layer3.2.bn1.bias', '_feature_blocks.layer3.2.bn1.running_mean', '_feature_blocks.layer3.2.bn1.running_var', '_feature_blocks.layer3.2.bn1.num_batches_tracked', '_feature_blocks.layer3.2.conv2.weight', '_feature_blocks.layer3.2.bn2.weight', '_feature_blocks.layer3.2.bn2.bias', '_feature_blocks.layer3.2.bn2.running_mean', '_feature_blocks.layer3.2.bn2.running_var', '_feature_blocks.layer3.2.bn2.num_batches_tracked', '_feature_blocks.layer3.2.conv3.weight', '_feature_blocks.layer3.2.bn3.weight', '_feature_blocks.layer3.2.bn3.bias', '_feature_blocks.layer3.2.bn3.running_mean', '_feature_blocks.layer3.2.bn3.running_var', '_feature_blocks.layer3.2.bn3.num_batches_tracked', '_feature_blocks.layer3.3.conv1.weight', '_feature_blocks.layer3.3.bn1.weight', '_feature_blocks.layer3.3.bn1.bias', '_feature_blocks.layer3.3.bn1.running_mean', '_feature_blocks.layer3.3.bn1.running_var', '_feature_blocks.layer3.3.bn1.num_batches_tracked', '_feature_blocks.layer3.3.conv2.weight', '_feature_blocks.layer3.3.bn2.weight', '_feature_blocks.layer3.3.bn2.bias', '_feature_blocks.layer3.3.bn2.running_mean', '_feature_blocks.layer3.3.bn2.running_var', '_feature_blocks.layer3.3.bn2.num_batches_tracked', '_feature_blocks.layer3.3.conv3.weight', '_feature_blocks.layer3.3.bn3.weight', '_feature_blocks.layer3.3.bn3.bias', '_feature_blocks.layer3.3.bn3.running_mean', '_feature_blocks.layer3.3.bn3.running_var', '_feature_blocks.layer3.3.bn3.num_batches_tracked', '_feature_blocks.layer3.4.conv1.weight', '_feature_blocks.layer3.4.bn1.weight', '_feature_blocks.layer3.4.bn1.bias', '_feature_blocks.layer3.4.bn1.running_mean', '_feature_blocks.layer3.4.bn1.running_var', '_feature_blocks.layer3.4.bn1.num_batches_tracked', '_feature_blocks.layer3.4.conv2.weight', '_feature_blocks.layer3.4.bn2.weight', '_feature_blocks.layer3.4.bn2.bias', '_feature_blocks.layer3.4.bn2.running_mean', '_feature_blocks.layer3.4.bn2.running_var', '_feature_blocks.layer3.4.bn2.num_batches_tracked', '_feature_blocks.layer3.4.conv3.weight', '_feature_blocks.layer3.4.bn3.weight', '_feature_blocks.layer3.4.bn3.bias', '_feature_blocks.layer3.4.bn3.running_mean', '_feature_blocks.layer3.4.bn3.running_var', '_feature_blocks.layer3.4.bn3.num_batches_tracked', '_feature_blocks.layer3.5.conv1.weight', '_feature_blocks.layer3.5.bn1.weight', '_feature_blocks.layer3.5.bn1.bias', '_feature_blocks.layer3.5.bn1.running_mean', '_feature_blocks.layer3.5.bn1.running_var', '_feature_blocks.layer3.5.bn1.num_batches_tracked', '_feature_blocks.layer3.5.conv2.weight', '_feature_blocks.layer3.5.bn2.weight', '_feature_blocks.layer3.5.bn2.bias', '_feature_blocks.layer3.5.bn2.running_mean', '_feature_blocks.layer3.5.bn2.running_var', '_feature_blocks.layer3.5.bn2.num_batches_tracked', '_feature_blocks.layer3.5.conv3.weight', '_feature_blocks.layer3.5.bn3.weight', '_feature_blocks.layer3.5.bn3.bias', '_feature_blocks.layer3.5.bn3.running_mean', '_feature_blocks.layer3.5.bn3.running_var', '_feature_blocks.layer3.5.bn3.num_batches_tracked', '_feature_blocks.layer4.0.conv1.weight', '_feature_blocks.layer4.0.bn1.weight', '_feature_blocks.layer4.0.bn1.bias', '_feature_blocks.layer4.0.bn1.running_mean', '_feature_blocks.layer4.0.bn1.running_var', '_feature_blocks.layer4.0.bn1.num_batches_tracked', '_feature_blocks.layer4.0.conv2.weight', '_feature_blocks.layer4.0.bn2.weight', '_feature_blocks.layer4.0.bn2.bias', '_feature_blocks.layer4.0.bn2.running_mean', '_feature_blocks.layer4.0.bn2.running_var', '_feature_blocks.layer4.0.bn2.num_batches_tracked', '_feature_blocks.layer4.0.conv3.weight', '_feature_blocks.layer4.0.bn3.weight', '_feature_blocks.layer4.0.bn3.bias', '_feature_blocks.layer4.0.bn3.running_mean', '_feature_blocks.layer4.0.bn3.running_var', '_feature_blocks.layer4.0.bn3.num_batches_tracked', '_feature_blocks.layer4.0.downsample.0.weight', '_feature_blocks.layer4.0.downsample.1.weight', '_feature_blocks.layer4.0.downsample.1.bias', '_feature_blocks.layer4.0.downsample.1.running_mean', '_feature_blocks.layer4.0.downsample.1.running_var', '_feature_blocks.layer4.0.downsample.1.num_batches_tracked', '_feature_blocks.layer4.1.conv1.weight', '_feature_blocks.layer4.1.bn1.weight', '_feature_blocks.layer4.1.bn1.bias', '_feature_blocks.layer4.1.bn1.running_mean', '_feature_blocks.layer4.1.bn1.running_var', '_feature_blocks.layer4.1.bn1.num_batches_tracked', '_feature_blocks.layer4.1.conv2.weight', '_feature_blocks.layer4.1.bn2.weight', '_feature_blocks.layer4.1.bn2.bias', '_feature_blocks.layer4.1.bn2.running_mean', '_feature_blocks.layer4.1.bn2.running_var', '_feature_blocks.layer4.1.bn2.num_batches_tracked', '_feature_blocks.layer4.1.conv3.weight', '_feature_blocks.layer4.1.bn3.weight', '_feature_blocks.layer4.1.bn3.bias', '_feature_blocks.layer4.1.bn3.running_mean', '_feature_blocks.layer4.1.bn3.running_var', '_feature_blocks.layer4.1.bn3.num_batches_tracked', '_feature_blocks.layer4.2.conv1.weight', '_feature_blocks.layer4.2.bn1.weight', '_feature_blocks.layer4.2.bn1.bias', '_feature_blocks.layer4.2.bn1.running_mean', '_feature_blocks.layer4.2.bn1.running_var', '_feature_blocks.layer4.2.bn1.num_batches_tracked', '_feature_blocks.layer4.2.conv2.weight', '_feature_blocks.layer4.2.bn2.weight', '_feature_blocks.layer4.2.bn2.bias', '_feature_blocks.layer4.2.bn2.running_mean', '_feature_blocks.layer4.2.bn2.running_var', '_feature_blocks.layer4.2.bn2.num_batches_tracked', '_feature_blocks.layer4.2.conv3.weight', '_feature_blocks.layer4.2.bn3.weight', '_feature_blocks.layer4.2.bn3.bias', '_feature_blocks.layer4.2.bn3.running_mean', '_feature_blocks.layer4.2.bn3.running_var', '_feature_blocks.layer4.2.bn3.num_batches_tracked'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading vissl checkpoint\")\n",
    "path_checkpoint = Path(\"/home/olivier/Documents/master/mp/checkpoints/sku110k/rotnet_full/model_final_checkpoint_phase104.torch\")\n",
    "ssl_checkpoint = torch.load(path_checkpoint)\n",
    "print(\"Checkpoint contains:\")\n",
    "dataframe_dict = dict()\n",
    "dataframe_dict[\"phase_idx\"] = ssl_checkpoint[\"phase_idx\"]\n",
    "dataframe_dict[\"iteration_num\"] = ssl_checkpoint[\"iteration_num\"]\n",
    "dataframe_dict[\"train_phase_idx\"] = ssl_checkpoint[\"train_phase_idx\"]\n",
    "dataframe_dict[\"iteration\"] = ssl_checkpoint[\"iteration\"]\n",
    "dataframe_dict[\"type\"] = ssl_checkpoint[\"type\"]\n",
    "df = pandas.DataFrame(data=dataframe_dict.values(), index=dataframe_dict.keys(),columns=[\"Value\"])\n",
    "display(df)\n",
    "if(\"loss\", \"classy_state_dict\" in ssl_checkpoint.keys()):\n",
    "    print(\"Checkpoint also contains elements loss and classy_state_dict\")\n",
    "\n",
    "#the weights of the trunk resnet network are stored in a nested dict:    \n",
    "print(ssl_checkpoint[\"classy_state_dict\"][\"base_model\"][\"model\"][\"trunk\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2dc577",
   "metadata": {},
   "source": [
    "## Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a399ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import pdb\n",
    "\n",
    "def extract_features(path):\n",
    "    image = Image.open(path)\n",
    "    # Convert images to RGB. This is important\n",
    "    # as the model was trained on RGB images.\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "    # Image transformation pipeline.\n",
    "    pipeline = transforms.Compose([\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "    ])\n",
    "    x = pipeline(image)\n",
    "\n",
    "    #unsqueeze adds a dim for batch size (with 1 element the entire input tensor of the image)\n",
    "    features = model(x.unsqueeze(0))\n",
    "    #pdb.set_trace()\n",
    "    features_shape = features[0].shape\n",
    "\n",
    "    #print(f\"Features extracted have the shape: { features_shape }\")\n",
    "    return features[0]\n",
    "\n",
    "savefile = open(\"fts.txt\",\"w\")\n",
    "path_to_CornerShop_crops = Path(\"/home/olivier/Documents/master/mp/CornerShop/CornerShop/crops\")\n",
    "\n",
    "# for crop_dir in os.listdir(path_to_CornerShop_crops):\n",
    "#     #look for all the folders that are classes of this dataset\n",
    "#     cdir = path_to_CornerShop_crops / crop_dir\n",
    "#     if( not(os.path.isdir(cdir)) ):\n",
    "#         continue #skip non-dirs, these are not classes\n",
    "#     for img in os.listdir(cdir):\n",
    "#         #look in all the cropfolders for images to get feature vectors from\n",
    "#         if( not(img.endswith(\".jpg\")) ):\n",
    "#             continue #skip non image files   \n",
    "#         #print(\"found image {}\".format(img))\n",
    "#         fts = extract_features(cdir / img) #get feature vector\n",
    "#         print(fts, file=savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf7072b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2048])\n",
      "['CawstonDry', 'CawstonDry', 'CawstonDry', 'MinuteMaidAppelPerzik', 'CarrefourSmoothieAardbeiBlauweBessen', 'CarrefourSmoothieAardbeiBlauweBessen', 'CarrefourSmoothieAardbeiBlauweBessen', 'CarrefourSmoothieAardbeiBlauweBessen', 'GiniZeroFles1,5L', 'GiniZeroFles1,5L', 'GiniZeroFles1,5L', 'TropicanaSanguinello', 'TropicanaSanguinello', 'TropicanaSanguinello', 'TropicanaSanguinello', 'TropicanaSanguinello', '7upLemon', '7upLemon', '7upLemon', '7upLemon']\n"
     ]
    }
   ],
   "source": [
    "img_paths = list(path_to_CornerShop_crops.glob(\"*/*.jpg\"))#**/*.jpg op alle dieptes van subdirs kijken en itereren\n",
    "\n",
    "labels = [p.parent.stem for p,_ in zip(img_paths,range(20)) ] #stem attr, naam zonder exentie, name attr met extentie\n",
    "fts_stack = torch.stack([extract_features(p).squeeze() for p,_ in zip(img_paths,range(20)) ])\n",
    "print(fts_stack.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e378bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 20])\n",
      "tensor([[ 87.3110,  83.9168,  87.3473, 109.3133, 135.7349,  77.9418,  95.8140,\n",
      "         120.6644,  75.7141, 139.5700,  75.3643, 109.6743, 179.2607,  81.3136,\n",
      "         126.0925,  98.3647, 115.1595,  96.6702, 104.6836,  94.0268],\n",
      "        [ 83.9168,  88.7915,  87.2473, 110.2363, 137.6209,  79.6288,  97.1384,\n",
      "         122.2782,  77.9874, 142.5118,  77.8924, 110.9518, 181.8811,  82.6621,\n",
      "         127.6149, 100.1078, 115.6713,  97.8970, 105.6707,  95.2109],\n",
      "        [ 87.3473,  87.2473,  93.0548, 112.7572, 141.2955,  81.3216,  99.6091,\n",
      "         125.5396,  79.4377, 145.5551,  79.1773, 114.1908, 187.3758,  84.7680,\n",
      "         131.0218, 102.5855, 119.3287, 100.4800, 108.6117,  97.5758],\n",
      "        [109.3133, 110.2363, 112.7572, 148.6957, 179.3886, 102.5920, 125.8214,\n",
      "         159.0141,  99.9860, 185.4647,  99.1786, 144.2331, 237.8656, 106.6190,\n",
      "         166.2170, 129.2002, 150.8825, 126.7883, 137.6863, 123.7811],\n",
      "        [135.7349, 137.6209, 141.2955, 179.3886, 232.1633, 128.4045, 158.3786,\n",
      "         202.2130, 124.9198, 233.4495, 124.4799, 181.0616, 302.5054, 133.6175,\n",
      "         210.8610, 163.1136, 189.9757, 158.6342, 173.2655, 154.5089],\n",
      "        [ 77.9418,  79.6288,  81.3216, 102.5920, 128.4045,  77.8767,  92.3711,\n",
      "         114.8642,  72.1480, 131.2299,  73.1324, 104.0968, 168.5009,  77.5691,\n",
      "         118.1694,  93.7675, 107.4699,  91.9152,  98.1032,  89.2180],\n",
      "        [ 95.8140,  97.1384,  99.6091, 125.8214, 158.3786,  92.3711, 115.2688,\n",
      "         141.6262,  87.7923, 162.1369,  88.2122, 128.2167, 209.1344,  94.7302,\n",
      "         146.1537, 114.8009, 132.7784, 112.1485, 121.0615, 108.8919],\n",
      "        [120.6644, 122.2782, 125.5396, 159.0141, 202.2130, 114.8642, 141.6262,\n",
      "         184.3035, 110.8952, 207.0183, 110.9743, 161.2627, 268.0931, 119.1010,\n",
      "         186.8258, 145.3573, 168.6594, 141.6546, 153.6655, 137.6802],\n",
      "        [ 75.7141,  77.9874,  79.4377,  99.9860, 124.9198,  72.1480,  87.7923,\n",
      "         110.8952,  73.7521, 129.0035,  71.1088, 100.4793, 163.9773,  75.4491,\n",
      "         116.2801,  91.0828, 104.7816,  88.8232,  95.9165,  86.4652],\n",
      "        [139.5700, 142.5118, 145.5551, 185.4647, 233.4495, 131.2299, 162.1369,\n",
      "         207.0183, 129.0035, 248.8703, 128.4194, 186.0280, 313.8064, 136.7892,\n",
      "         216.9355, 167.1249, 196.0355, 162.5442, 178.5393, 158.5695],\n",
      "        [ 75.3643,  77.8924,  79.1773,  99.1786, 124.4799,  73.1324,  88.2122,\n",
      "         110.9743,  71.1088, 128.4194,  73.9994, 100.9082, 163.6844,  75.6522,\n",
      "         115.2666,  91.0940, 104.4125,  89.4714,  95.5014,  86.6579],\n",
      "        [109.6743, 110.9518, 114.1908, 144.2331, 181.0616, 104.0968, 128.2167,\n",
      "         161.2627, 100.4793, 186.0280, 100.9082, 150.7784, 240.6302, 109.3641,\n",
      "         168.7609, 132.3150, 152.1656, 127.9149, 138.6847, 124.2893],\n",
      "        [179.2607, 181.8811, 187.3758, 237.8656, 302.5054, 168.5009, 209.1344,\n",
      "         268.0931, 163.9773, 313.8064, 163.6844, 240.6302, 417.2492, 176.5069,\n",
      "         281.1864, 216.1799, 253.2621, 209.3249, 230.3576, 203.9815],\n",
      "        [ 81.3136,  82.6621,  84.7680, 106.6190, 133.6175,  77.5691,  94.7302,\n",
      "         119.1010,  75.4491, 136.7892,  75.6522, 109.3641, 176.5069,  84.0746,\n",
      "         124.4959,  98.3256, 112.4767,  95.1360, 102.5554,  92.4566],\n",
      "        [126.0925, 127.6149, 131.0218, 166.2170, 210.8610, 118.1694, 146.1537,\n",
      "         186.8258, 116.2801, 216.9355, 115.2666, 168.7609, 281.1864, 124.4959,\n",
      "         201.2207, 152.0244, 176.8633, 146.6231, 161.2026, 143.0023],\n",
      "        [ 98.3647, 100.1078, 102.5855, 129.2002, 163.1136,  93.7675, 114.8009,\n",
      "         145.3573,  91.0828, 167.1249,  91.0940, 132.3150, 216.1799,  98.3256,\n",
      "         152.0244, 122.4430, 136.9356, 115.2412, 125.2432, 112.0270],\n",
      "        [115.1595, 115.6713, 119.3287, 150.8825, 189.9757, 107.4699, 132.7784,\n",
      "         168.6594, 104.7816, 196.0355, 104.4125, 152.1656, 253.2621, 112.4767,\n",
      "         176.8633, 136.9356, 165.5786, 133.6370, 146.4680, 130.3102],\n",
      "        [ 96.6702,  97.8970, 100.4800, 126.7883, 158.6342,  91.9152, 112.1485,\n",
      "         141.6546,  88.8232, 162.5442,  89.4714, 127.9149, 209.3249,  95.1360,\n",
      "         146.6231, 115.2412, 133.6370, 116.9146, 121.8334, 111.8834],\n",
      "        [104.6836, 105.6707, 108.6117, 137.6862, 173.2655,  98.1032, 121.0615,\n",
      "         153.6655,  95.9165, 178.5393,  95.5014, 138.6847, 230.3576, 102.5554,\n",
      "         161.2026, 125.2432, 146.4680, 121.8334, 137.0722, 118.6854],\n",
      "        [ 94.0268,  95.2109,  97.5758, 123.7811, 154.5089,  89.2180, 108.8919,\n",
      "         137.6802,  86.4652, 158.5695,  86.6579, 124.2893, 203.9815,  92.4566,\n",
      "         143.0023, 112.0270, 130.3102, 111.8834, 118.6854, 110.5263]])\n"
     ]
    }
   ],
   "source": [
    "cosin_sim = fts_stack.matmul(fts_stack.T)\n",
    "print(cosin_sim.shape)\n",
    "print(cosin_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ee6222e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9315)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fts_stack_norm = fts_stack / fts_stack.norm(dim=1).unsqueeze(1)\n",
    "cosim = fts_stack_norm.matmul(fts_stack_norm.T)\n",
    "cosim.min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
