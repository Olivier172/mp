{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8736c055",
   "metadata": {},
   "source": [
    "# Validation SSL model\n",
    "\n",
    "In this notebook our goal is to test how good our SSL pretrained weights are. \n",
    "- We will query images from different classes and compare embeddings. This will give us better insights for the intraclass/interclass variability.\n",
    "    - Intraclass variance: variance within one class (The intraclass variance measures the differences between the individual embeddings within each class.)\n",
    "    - Interclass variance: variance between different classes (The interclass variance measures the differences between the means of each class)\n",
    "- Note: you need to run this notebook with a kernel in your venv to use vissl libs: https://janakiev.com/blog/jupyter-virtual-envs/#add-virtual-environment-to-jupyter-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6c712",
   "metadata": {},
   "source": [
    "## Imports\n",
    "- matplotlib for visualisation\n",
    "- torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ebd1b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df5f44e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ebec0",
   "metadata": {},
   "source": [
    "## Reading in pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1105b3d",
   "metadata": {},
   "source": [
    "### Option 1: Imagenet pretrained\n",
    "- Load the best imgnet pretrained weights, docs: https://pytorch.org/vision/stable/models.html\n",
    "- This is currently ResNet50_Weights.IMAGENET1K_V2 with an accuracy of 80.858%\n",
    "- weights are saved in /home/olivier/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "523d9938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /home/olivier/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ef1c6d2b8c4e869028e568141f6230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#imgnet weights\n",
    "#model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "#torch.save(model.state_dict(),\"resnet50_imgnet.pth\")\n",
    "#weights = torch.load(\"resnet50_imgnet.pth\")\n",
    "#print(weights.keys())\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4716f403",
   "metadata": {},
   "source": [
    "### Option 2: SSL pretrained\n",
    "Load weights from checkpoint according to vissl tutorial:\n",
    "https://github.com/facebookresearch/vissl/blob/v0.1.6/tutorials/Using_a_pretrained_model_for_inference_V0_1_6.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa1d21ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train config at (relative path from vissl/...):\n",
      "validation/rotnet_full/train_config.yaml\n",
      "SSL pretrained weights at:\n",
      "/home/olivier/Documents/master/mp/checkpoints/sku110k/rotnet_full/model_final_checkpoint_phase104.torch\n"
     ]
    }
   ],
   "source": [
    "#dictionary to summarize the paths to the the training config used and the path to the weigths\n",
    "#train_config path is a relative path from the vissl folder\n",
    "#weights path is an absolute path to where the final_checkpoint.torch is stored \n",
    "PATHS = {\n",
    "    \"rotnet\":\n",
    "    {\n",
    "        \"train_config\": \"validation/rotnet_full/train_config.yaml\", #relative path from vissl/...\n",
    "        \"weights\": \"/home/olivier/Documents/master/mp/checkpoints/sku110k/rotnet_full/model_final_checkpoint_phase104.torch\",\n",
    "    },\n",
    "    \"jigsaw\":\n",
    "    {\n",
    "        \"train_config\": \"validation/jigsaw_full/train_config.yaml\",\n",
    "        \"weights\": \"/home/olivier/Documents/master/mp/checkpoints/sku110k/jigsaw_full/model_final_checkpoint_phase104.torch\"\n",
    "    },\n",
    "    \"moco\":\n",
    "    {\n",
    "        \"train_config\": \"validation/moco_full/train_config.yaml\",\n",
    "        \"weights\": \"/home/olivier/Documents/master/mp/checkpoints/sku110k/moco_full/model_final_checkpoint_phase199.torch\"\n",
    "    },\n",
    "    \"simclr\":\n",
    "    {\n",
    "        \"train_config\": \"\",\n",
    "        \"weights\": \"\"\n",
    "    },\n",
    "    \"swav\":\n",
    "    {\n",
    "        \"train_config\": \"\",\n",
    "        \"weights\": \"\"\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "#CHOOSE the model you want to validate here\n",
    "train_config = PATHS[\"rotnet\"][\"train_config\"] #change the key of the PATHS dict to the desired model name\n",
    "weights_file = PATHS[\"rotnet\"][\"weights\"]\n",
    "print('Train config at (relative path from vissl/...):\\n' + train_config)\n",
    "print('SSL pretrained weights at:\\n' + weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3282ada2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.common.file_io:** fvcore version of PathManager will be deprecated soon. **\n",
      "** Please migrate to the version in iopath repo. **\n",
      "https://github.com/facebookresearch/iopath \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from vissl.utils.hydra_config import AttrDict\n",
    "from vissl.utils.hydra_config import compose_hydra_configuration, convert_to_attrdict\n",
    "\n",
    "# 1. Checkpoint config is located at vissl/configs/config/validation/*/train_config.yaml.\n",
    "# 2. weights are located at /home/olivier/Documents/master/mp/checkpoints/sku110k/*\n",
    "# The * in the above paths stand for rotnet_full, jigsaw_full or moco_full\n",
    "# All other options specified below override the train_config.yaml config.\n",
    "\n",
    "cfg = [\n",
    "  'config=' + train_config,\n",
    "  'config.MODEL.WEIGHTS_INIT.PARAMS_FILE=' + weights_file, # Specify path for the model weights.\n",
    "  'config.MODEL.FEATURE_EVAL_SETTINGS.EVAL_MODE_ON=True', # Turn on model evaluation mode.\n",
    "  'config.MODEL.FEATURE_EVAL_SETTINGS.FREEZE_TRUNK_ONLY=True', # Freeze trunk. \n",
    "  'config.MODEL.FEATURE_EVAL_SETTINGS.EXTRACT_TRUNK_FEATURES_ONLY=True', # Extract the trunk features, as opposed to the HEAD.\n",
    "  'config.MODEL.FEATURE_EVAL_SETTINGS.SHOULD_FLATTEN_FEATS=False', # Do not flatten features.\n",
    "  'config.MODEL.FEATURE_EVAL_SETTINGS.LINEAR_EVAL_FEAT_POOL_OPS_MAP=[[\"res5avg\", [\"Identity\", []]]]' # Extract only the res5avg features.\n",
    "]\n",
    "\n",
    "# Compose the hydra configuration.\n",
    "cfg = compose_hydra_configuration(cfg)\n",
    "# Convert to AttrDict. This method will also infer certain config options\n",
    "# and validate the config is valid.\n",
    "_, cfg = convert_to_attrdict(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6e3f0",
   "metadata": {},
   "source": [
    "Now let's build the model with the exact training configs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce794485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vissl.models import build_model\n",
    "\n",
    "model = build_model(cfg.MODEL, cfg.OPTIMIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a322b7",
   "metadata": {},
   "source": [
    "#### Loading the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1b5622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights have loaded\n"
     ]
    }
   ],
   "source": [
    "from classy_vision.generic.util import load_checkpoint\n",
    "from vissl.utils.checkpoint import init_model_from_consolidated_weights\n",
    "\n",
    "# Load the checkpoint weights.\n",
    "weights = load_checkpoint(checkpoint_path=cfg.MODEL.WEIGHTS_INIT.PARAMS_FILE)\n",
    "\n",
    "\n",
    "# Initializei the model with the simclr model weights.\n",
    "init_model_from_consolidated_weights(\n",
    "    config=cfg,\n",
    "    model=model,\n",
    "    state_dict=weights,\n",
    "    state_dict_key_name=\"classy_state_dict\",\n",
    "    skip_layers=[],  # Use this if you do not want to load all layers\n",
    ")\n",
    "\n",
    "print(\"Weights have loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9120f9",
   "metadata": {},
   "source": [
    "#### Extra info\n",
    "- VISSL uses the ResNeXT50 class, which is their custom wrapper class\n",
    "    - ResNeXT50 wrapper class is defined at https://github.com/facebookresearch/vissl/blob/04788de934b39278326331f7a4396e03e85f6e55/vissl/models/trunks/resnext.py\n",
    "    - ResNet base class https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py for interface of the __init__ method.\n",
    "    - the model of this wrapper class is a torchvision.models.ResNet() which we will reconstruct here based on the YAML config parameters.\n",
    "- checkpoints from pretraining are stored on /home/olivier/Documents/master/mp/checkpoints/sku110k/\n",
    "    - checkpoints have phase numbers: in VISSL, if the workflow involves training and testing both, the number of phases = train phases + test epochs. So if we alternate train and test, the phase number is: 0 (train), 1 (test), 2 (train), 3 (test)... and train_phase_idx is always: 0 (corresponds to phase0), 1 (correponds to phase 2)\n",
    "    - The weights are stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7238a7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vissl checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olivier/Documents/master/mp/.venv/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading vissl checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m ssl_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint contains:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m dataframe_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/master/mp/.venv/lib/python3.8/site-packages/torch/serialization.py:592\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    590\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    591\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 592\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/Documents/master/mp/.venv/lib/python3.8/site-packages/torch/serialization.py:851\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    849\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mUnpickler(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m    850\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m--> 851\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/master/mp/.venv/lib/python3.8/site-packages/torch/serialization.py:843\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    841\u001b[0m data_type, key, location, size \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[0;32m--> 843\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m storage \u001b[38;5;241m=\u001b[39m loaded_storages[key]\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m storage\n",
      "File \u001b[0;32m~/Documents/master/mp/.venv/lib/python3.8/site-packages/torch/serialization.py:832\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    829\u001b[0m dtype \u001b[38;5;241m=\u001b[39m data_type(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    831\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, size, dtype)\u001b[38;5;241m.\u001b[39mstorage()\n\u001b[0;32m--> 832\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/master/mp/.venv/lib/python3.8/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/master/mp/.venv/lib/python3.8/site-packages/torch/serialization.py:151\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 151\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    153\u001b[0m             storage_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda, \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/master/mp/.venv/lib/python3.8/site-packages/torch/serialization.py:135\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    132\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    136\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    137\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    138\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    139\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    140\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "print(\"Loading vissl checkpoint\")\n",
    "ssl_checkpoint = torch.load(Path(weights_file))\n",
    "print(\"Checkpoint contains:\")\n",
    "dataframe_dict = dict()\n",
    "dataframe_dict[\"phase_idx\"] = ssl_checkpoint[\"phase_idx\"]\n",
    "dataframe_dict[\"iteration_num\"] = ssl_checkpoint[\"iteration_num\"]\n",
    "dataframe_dict[\"train_phase_idx\"] = ssl_checkpoint[\"train_phase_idx\"]\n",
    "dataframe_dict[\"iteration\"] = ssl_checkpoint[\"iteration\"]\n",
    "dataframe_dict[\"type\"] = ssl_checkpoint[\"type\"]\n",
    "df = pd.DataFrame(data=dataframe_dict.values(), index=dataframe_dict.keys(),columns=[\"Value\"])\n",
    "display(df)\n",
    "if(\"loss\", \"classy_state_dict\" in ssl_checkpoint.keys()):\n",
    "    print(\"Checkpoint also contains elements loss and classy_state_dict\")\n",
    "\n",
    "#the weights of the trunk resnet network are stored in a nested dict:    \n",
    "#print(ssl_checkpoint[\"classy_state_dict\"][\"base_model\"][\"model\"][\"trunk\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2dc577",
   "metadata": {},
   "source": [
    "## Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a399ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def extract_features(path):\n",
    "    image = Image.open(path)\n",
    "    # Convert images to RGB. This is important\n",
    "    # as the model was trained on RGB images.\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "    # Image transformation pipeline.\n",
    "    pipeline = transforms.Compose([\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "    ])\n",
    "    x = pipeline(image)\n",
    "\n",
    "    #unsqueeze adds a dim for batch size (with 1 element the entire input tensor of the image)\n",
    "    features = model(x.unsqueeze(0))\n",
    "\n",
    "    features_shape = features[0].shape\n",
    "    #print(f\"Features extracted have the shape: { features_shape }\")\n",
    "    return features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf7072b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2048])\n",
      "['CawstonDry', 'CawstonDry', 'CawstonDry', 'MinuteMaidAppelPerzik', 'CarrefourSmoothieAardbeiBlauweBessen', 'CarrefourSmoothieAardbeiBlauweBessen', 'CarrefourSmoothieAardbeiBlauweBessen', 'CarrefourSmoothieAardbeiBlauweBessen', 'GiniZeroFles1,5L', 'GiniZeroFles1,5L']\n"
     ]
    }
   ],
   "source": [
    "savefile = open(\"fts.txt\",\"w\")\n",
    "CornerShop = Path(\"/home/olivier/Documents/master/mp/CornerShop/CornerShop/crops\")\n",
    "\n",
    "#create an iterator over all jpg files in cornershop map and put elements in a list\n",
    "img_paths = list(CornerShop.glob(\"*/*.jpg\")) #**/*.jpg to look into all subdirs for jpgs and iterate over them\n",
    "#extract the corresponding labels (folder names)\n",
    "labels = [p.parent.stem for p,_ in zip(img_paths,range(20)) ] #stem attr, conatins foldername \n",
    "#path.stem=filename without extension\n",
    "#path.name=filename with extension\n",
    "fts_stack = torch.stack([extract_features(p).squeeze() for p,_ in zip(img_paths,range(20)) ])\n",
    "print(fts_stack.shape)\n",
    "print(labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f9c299",
   "metadata": {},
   "source": [
    "results of the feature extraction:\n",
    "- fts_stack: contains n rows and 2048 columns (features), this is a stack of features from multiple query images\n",
    "- labels: list with the corresponding labels of the feature stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea772812",
   "metadata": {},
   "source": [
    "## Comparing features\n",
    "Here we will investigate relations between the features from different images with:\n",
    "- Inner product\n",
    "- Cosine simularity\n",
    "- Euclidian distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a61c4",
   "metadata": {},
   "source": [
    "### Inner product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f8de8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three examples from inner_product tensor:\n",
      "tensor([[ 87.3110,  83.9168,  87.3473, 109.3133, 135.7349,  77.9418,  95.8140,\n",
      "         120.6644,  75.7141, 139.5700,  75.3643, 109.6743, 179.2607,  81.3136,\n",
      "         126.0925,  98.3647, 115.1595,  96.6702, 104.6836,  94.0268],\n",
      "        [ 83.9168,  88.7915,  87.2473, 110.2363, 137.6209,  79.6288,  97.1384,\n",
      "         122.2782,  77.9874, 142.5118,  77.8924, 110.9518, 181.8811,  82.6621,\n",
      "         127.6149, 100.1078, 115.6713,  97.8970, 105.6707,  95.2109],\n",
      "        [ 87.3473,  87.2473,  93.0548, 112.7572, 141.2955,  81.3216,  99.6091,\n",
      "         125.5396,  79.4377, 145.5551,  79.1773, 114.1908, 187.3758,  84.7680,\n",
      "         131.0218, 102.5855, 119.3287, 100.4800, 108.6117,  97.5758]])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inner_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>417.249237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>71.108810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>132.755661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_dev</th>\n",
       "      <td>47.856178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         inner_product\n",
       "max         417.249237\n",
       "min          71.108810\n",
       "avg         132.755661\n",
       "std_dev      47.856178"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#multiply the features of one tensor with all other tensors\n",
    "inner_product = fts_stack.matmul(fts_stack.T)\n",
    "print(\"Three examples from inner_product tensor:\\n{}\".format(inner_product[0:3]))\n",
    "\n",
    "#data analytics\n",
    "data = np.zeros((4))\n",
    "#data\n",
    "data[0]= inner_product.max()\n",
    "data[1]= inner_product.min()\n",
    "data[2]= torch.mean(inner_product)\n",
    "data[3]= torch.std(inner_product)\n",
    "#labels\n",
    "data_labels=[\"max\",\"min\",\"avg\",\"std_dev\"]\n",
    "\n",
    "df = pd.DataFrame(data=data,index=data_labels, columns=[\"inner_product\"])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca8ac4",
   "metadata": {},
   "source": [
    "### Cosine simularity\n",
    "Here we normalize the features and then calculate the inner product with all other tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee6222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZE features in feature stack:\n",
    "fts_stack_norm = fts_stack / fts_stack.norm(dim=1,keepdim=True) \n",
    "#fts.norm(dim=1,keepdim=True)\n",
    "# dim=1: calculate norm over the second dimension (features/columns)\n",
    "# keepdim=True: keep batch/stack dimension of features\n",
    "\n",
    "#.norm is deprecated, newer version https://pytorch.org/docs/stable/generated/torch.linalg.matrix_norm.html#torch.linalg.matrix_norm \n",
    "#fts_stack_norm = fts_stack / torch.linalg.matrix_norm(fts_stack, dim=1, keepdim=True) #newer version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f83a914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three examples from cosim tensor:\n",
      "tensor([[1.0000, 0.9531, 0.9690, 0.9594, 0.9534, 0.9452, 0.9551, 0.9512, 0.9435,\n",
      "         0.9468, 0.9376, 0.9559, 0.9392, 0.9491, 0.9513, 0.9513, 0.9578, 0.9568,\n",
      "         0.9569, 0.9572],\n",
      "        [0.9531, 1.0000, 0.9598, 0.9594, 0.9585, 0.9576, 0.9602, 0.9559, 0.9637,\n",
      "         0.9587, 0.9609, 0.9589, 0.9449, 0.9567, 0.9547, 0.9601, 0.9540, 0.9608,\n",
      "         0.9578, 0.9611],\n",
      "        [0.9690, 0.9598, 1.0000, 0.9586, 0.9613, 0.9553, 0.9618, 0.9586, 0.9589,\n",
      "         0.9565, 0.9542, 0.9640, 0.9509, 0.9584, 0.9575, 0.9611, 0.9613, 0.9633,\n",
      "         0.9617, 0.9621]])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cosim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.931526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>0.961039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_dev</th>\n",
       "      <td>0.012083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cosim\n",
       "max      1.000001\n",
       "min      0.931526\n",
       "avg      0.961039\n",
       "std_dev  0.012083"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#calculate cosine simularity (cosim)\n",
    "#fts_stack is a matrix with n rows and 2048 columns (features)\n",
    "#matrix product of fts_stack * fts_stack^T = cosin_sim with all other images from the stack\n",
    "cosim = fts_stack_norm.matmul(fts_stack_norm.T)\n",
    "print(\"Three examples from cosim tensor:\\n{}\".format(cosim[0:3]))\n",
    "\n",
    "#data analytics\n",
    "data = np.zeros((4))\n",
    "#data\n",
    "data[0]= cosim.max()\n",
    "data[1]= cosim.min()\n",
    "data[2]= torch.mean(cosim)\n",
    "data[3]= torch.std(cosim)\n",
    "#labels\n",
    "data_labels=[\"max\",\"min\",\"avg\",\"std_dev\"]\n",
    "\n",
    "df = pd.DataFrame(data=data,index=data_labels, columns=[\"cosim\"])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964761b3",
   "metadata": {},
   "source": [
    "### Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecd061cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three examples from eucl_dist tensor:\n",
      "tensor([[ 0.0000,  2.8756,  2.3814,  4.1689,  6.9285,  3.0503,  3.3094,  5.5032,\n",
      "          3.1040,  7.5526,  3.2529,  4.3291, 12.0847,  2.9595,  6.0288,  3.6089,\n",
      "          4.7509,  3.2993,  3.8750,  3.1279],\n",
      "        [ 2.8756,  0.0000,  2.7114,  4.1249,  6.7611,  2.7222,  3.1279,  5.3422,\n",
      "          2.5630,  7.2552,  2.6469,  4.2031, 11.9281,  2.7462,  5.8977,  3.3195,\n",
      "          4.7987,  3.1484,  3.8108,  2.9826],\n",
      "        [ 2.3814,  2.7114,  0.0000,  4.0294,  6.5289,  2.8789,  3.0175,  5.1263,\n",
      "          2.8163,  7.1285,  2.9495,  3.9309, 11.6427,  2.7556,  5.6773,  3.2135,\n",
      "          4.4695,  3.0016,  3.5922,  2.9034]])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eucl_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.801560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>4.611862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_dev</th>\n",
       "      <td>2.495889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         eucl_dist\n",
       "max      12.801560\n",
       "min       0.000000\n",
       "avg       4.611862\n",
       "std_dev   2.495889"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eucl_dist = [] \n",
    "for tensor in fts_stack:\n",
    "    d = [] #store all distances from this tensor to all the other tensors\n",
    "    for other_tensor in fts_stack:\n",
    "        d_to = (tensor - other_tensor).pow(2).sum().sqrt() #d(tensor, other_tensor)=euclid distance\n",
    "        d.append(d_to)\n",
    "    d = torch.tensor(d)\n",
    "    #print(\"distance tensor has shape {}\".format(d.shape))\n",
    "    #add tensor to euclidian distances \n",
    "    eucl_dist.append(d)\n",
    "eucl_dist = torch.stack(eucl_dist)\n",
    "#print(\"eucl_dist has shape {}\".format(eucl_dist.shape))\n",
    "print(\"Three examples from eucl_dist tensor:\\n{}\".format(eucl_dist[0:3]))\n",
    "\n",
    "#data analytics\n",
    "data = np.zeros((4))\n",
    "#data\n",
    "data[0]= eucl_dist.max()\n",
    "data[1]= eucl_dist.min()\n",
    "data[2]= torch.mean(eucl_dist)\n",
    "data[3]= torch.std(eucl_dist)\n",
    "#labels\n",
    "data_labels=[\"max\",\"min\",\"avg\",\"std_dev\"]\n",
    "\n",
    "df = pd.DataFrame(data=data,index=data_labels, columns=[\"eucl_dist\"])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfcf7c4",
   "metadata": {},
   "source": [
    "### Euclidian distance (normalized features)\n",
    "Using normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17bda747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three examples from eucl_dist tensor:\n",
      "tensor([[0.0000, 0.3063, 0.2488, 0.2850, 0.3054, 0.3310, 0.2997, 0.3124, 0.3361,\n",
      "         0.3261, 0.3533, 0.2971, 0.3487, 0.3192, 0.3121, 0.3119, 0.2906, 0.2939,\n",
      "         0.2936, 0.2927],\n",
      "        [0.3063, 0.0000, 0.2834, 0.2850, 0.2880, 0.2912, 0.2822, 0.2971, 0.2694,\n",
      "         0.2874, 0.2795, 0.2867, 0.3318, 0.2942, 0.3009, 0.2825, 0.3034, 0.2799,\n",
      "         0.2904, 0.2789],\n",
      "        [0.2488, 0.2834, 0.0000, 0.2878, 0.2782, 0.2990, 0.2765, 0.2877, 0.2867,\n",
      "         0.2951, 0.3028, 0.2682, 0.3133, 0.2886, 0.2916, 0.2791, 0.2781, 0.2708,\n",
      "         0.2768, 0.2752]])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eucl_dist_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.370063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>0.270670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_dev</th>\n",
       "      <td>0.068351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         eucl_dist_norm\n",
       "max            0.370063\n",
       "min            0.000000\n",
       "avg            0.270670\n",
       "std_dev        0.068351"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eucl_dist_norm = [] \n",
    "for tensor in fts_stack_norm:\n",
    "    d = [] #store all distances from this tensor to all the other tensors\n",
    "    for other_tensor in fts_stack_norm:\n",
    "        d_to = (tensor - other_tensor).pow(2).sum().sqrt() #d(tensor, other_tensor)=euclid distance\n",
    "        d.append(d_to)\n",
    "    d = torch.tensor(d)\n",
    "    #print(\"distance tensor has shape {}\".format(d.shape))\n",
    "    #add tensor to euclidian distances \n",
    "    eucl_dist_norm.append(d)\n",
    "eucl_dist_norm = torch.stack(eucl_dist_norm)\n",
    "#print(\"eucl_dist has shape {}\".format(eucl_dist.shape))\n",
    "print(\"Three examples from eucl_dist tensor:\\n{}\".format(eucl_dist_norm[0:3]))\n",
    "\n",
    "#data analytics\n",
    "data = np.zeros((4))\n",
    "#data\n",
    "data[0]= eucl_dist_norm.max()\n",
    "data[1]= eucl_dist_norm.min()\n",
    "data[2]= torch.mean(eucl_dist_norm)\n",
    "data[3]= torch.std(eucl_dist_norm)\n",
    "#labels\n",
    "data_labels=[\"max\",\"min\",\"avg\",\"std_dev\"]\n",
    "\n",
    "df = pd.DataFrame(data=data,index=data_labels, columns=[\"eucl_dist_norm\"])\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
