{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8736c055",
   "metadata": {},
   "source": [
    "# Embedding Characteristics\n",
    "\n",
    "In this notebook our goal is to test how good our SSL pretrained weights are. \n",
    "- We will query images from different classes and compare embeddings. This will give us better insights for the intraclass/interclass variability.\n",
    "    - Intraclass variance: variance within one class (The intraclass variance measures the differences between the individual embeddings within each class.)\n",
    "    - Interclass variance: variance between different classes (The interclass variance measures the differences between the means of each class)\n",
    "- Note: you need to run this notebook with a kernel in your venv to use vissl libs: https://janakiev.com/blog/jupyter-virtual-envs/#add-virtual-environment-to-jupyter-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6c712",
   "metadata": {},
   "source": [
    "## Imports\n",
    "- matplotlib for visualisation\n",
    "- torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ebd1b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df5f44e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ebec0",
   "metadata": {},
   "source": [
    "## Reading in pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1105b3d",
   "metadata": {},
   "source": [
    "### Option 1: Imagenet pretrained\n",
    "- Load the best imgnet pretrained weights, docs: https://pytorch.org/vision/stable/models.html\n",
    "- This is currently ResNet50_Weights.IMAGENET1K_V2 with an accuracy of 80.858%\n",
    "- weights are saved in /home/olivier/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "523d9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgnet weights\n",
    "#model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "# model = torchvision.models.resnet50(pretrained=True)\n",
    "#torch.save(model.state_dict(),\"resnet50_imgnet.pth\")\n",
    "#weights = torch.load(\"resnet50_imgnet.pth\")\n",
    "#print(weights.keys())\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4716f403",
   "metadata": {},
   "source": [
    "### Option 2: SSL pretrained\n",
    "Load weights from checkpoint according to vissl tutorial:\n",
    "https://github.com/facebookresearch/vissl/blob/v0.1.6/tutorials/Using_a_pretrained_model_for_inference_V0_1_6.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa1d21ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train config at (relative path from vissl/...):\n",
      "validation/moco_full_64/train_config.yaml\n",
      "SSL pretrained weights at:\n",
      "/home/olivier/Documents/master/mp/checkpoints/sku110k/moco_full_64/model_final_checkpoint_phase99.torch\n"
     ]
    }
   ],
   "source": [
    "#dictionary to summarize the paths to the the training config used and the path to the weigths\n",
    "#train_config path is a relative path from the vissl folder\n",
    "#weights path is an absolute path to where the final_checkpoint.torch is stored \n",
    "BASE_DIR_WEIGHTS = \"/home/olivier/Documents/master/mp/checkpoints/\"\n",
    "PATHS = {\n",
    "    \"rotnet\":\n",
    "    {\n",
    "        \"train_config\": \"validation/rotnet_full/train_config.yaml\", #relative path from vissl/...\n",
    "        \"weights\": BASE_DIR_WEIGHTS + \"sku110k/rotnet_full/model_final_checkpoint_phase104.torch\",\n",
    "    },\n",
    "    \"jigsaw\":\n",
    "    {\n",
    "        \"train_config\": \"validation/jigsaw_full/train_config.yaml\",\n",
    "        \"weights\": BASE_DIR_WEIGHTS + \"sku110k/jigsaw_full/model_final_checkpoint_phase104.torch\"\n",
    "    },\n",
    "    \"moco32\":\n",
    "    {\n",
    "        \"train_config\": \"validation/moco_full_32/train_config.yaml\",\n",
    "        \"weights\": BASE_DIR_WEIGHTS + \"sku110k/moco_full_32/model_final_checkpoint_phase99.torch\"\n",
    "    },\n",
    "    \"moco64\":\n",
    "    {\n",
    "        \"train_config\": \"validation/moco_full_64/train_config.yaml\",\n",
    "        \"weights\": BASE_DIR_WEIGHTS + \"sku110k/moco_full_64/model_final_checkpoint_phase99.torch\"\n",
    "    },\n",
    "    \"simclr\":\n",
    "    {\n",
    "        \"train_config\": \"validation/simclr_full/train_config.yaml\",\n",
    "        \"weights\": BASE_DIR_WEIGHTS + \"sku110k/simclr_full/model_final_checkpoint_phase99.torch\"\n",
    "    },\n",
    "    \"swav\":\n",
    "    {\n",
    "        \"train_config\": \"validation/swav_full/train_config.yaml\",\n",
    "        \"weights\": BASE_DIR_WEIGHTS + \"sku110k/swav_full/model_final_checkpoint_phase99.torch\"\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "#CHOOSE the model you want to validate here\n",
    "train_config = PATHS[\"moco64\"][\"train_config\"] #change the key of the PATHS dict to the desired model name\n",
    "weights_file = PATHS[\"moco64\"][\"weights\"]\n",
    "print('Train config at (relative path from vissl/...):\\n' + train_config)\n",
    "print('SSL pretrained weights at:\\n' + weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3282ada2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olivier/Documents/master/mp/.venv/lib/python3.8/site-packages/hydra/plugins/config_source.py:190: UserWarning: \n",
      "Missing @package directive config/validation/moco_full_64/train_config.yaml in pkg://configs.\n",
      "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/adding_a_package_directive\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "WARNING:fvcore.common.file_io:** fvcore version of PathManager will be deprecated soon. **\n",
      "** Please migrate to the version in iopath repo. **\n",
      "https://github.com/facebookresearch/iopath \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from vissl.utils.hydra_config import AttrDict\n",
    "from vissl.utils.hydra_config import compose_hydra_configuration, convert_to_attrdict\n",
    "\n",
    "# 1. Checkpoint config is located at vissl/configs/config/validation/*/train_config.yaml.\n",
    "# 2. weights are located at /home/olivier/Documents/master/mp/checkpoints/sku110k/*\n",
    "# The * in the above paths stand for rotnet_full, jigsaw_full or moco_full\n",
    "# All other options specified below override the train_config.yaml config.\n",
    "\n",
    "cfg = [\n",
    "  'config=' + train_config,\n",
    "  'config.MODEL.WEIGHTS_INIT.PARAMS_FILE=' + weights_file, # Specify path for the model weights.\n",
    "  'config.MODEL.FEATURE_EVAL_SETTINGS.EVAL_MODE_ON=True', # Turn on model evaluation mode.\n",
    "  'config.MODEL.FEATURE_EVAL_SETTINGS.FREEZE_TRUNK_ONLY=True', # Freeze trunk. \n",
    "  'config.MODEL.FEATURE_EVAL_SETTINGS.EXTRACT_TRUNK_FEATURES_ONLY=True', # Extract the trunk features, as opposed to the HEAD.\n",
    "  'config.MODEL.FEATURE_EVAL_SETTINGS.SHOULD_FLATTEN_FEATS=False', # Do not flatten features.\n",
    "  'config.MODEL.FEATURE_EVAL_SETTINGS.LINEAR_EVAL_FEAT_POOL_OPS_MAP=[[\"res5avg\", [\"Identity\", []]]]' # Extract only the res5avg features.\n",
    "]\n",
    "\n",
    "# Compose the hydra configuration.\n",
    "cfg = compose_hydra_configuration(cfg)\n",
    "# Convert to AttrDict. This method will also infer certain config options\n",
    "# and validate the config is valid.\n",
    "_, cfg = convert_to_attrdict(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6e3f0",
   "metadata": {},
   "source": [
    "Now let's build the model with the exact training configs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce794485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vissl.models import build_model\n",
    "\n",
    "model = build_model(cfg.MODEL, cfg.OPTIMIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a322b7",
   "metadata": {},
   "source": [
    "#### Loading the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b5622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights have loaded\n"
     ]
    }
   ],
   "source": [
    "from classy_vision.generic.util import load_checkpoint\n",
    "from vissl.utils.checkpoint import init_model_from_consolidated_weights\n",
    "\n",
    "# Load the checkpoint weights.\n",
    "weights = load_checkpoint(checkpoint_path=cfg.MODEL.WEIGHTS_INIT.PARAMS_FILE)\n",
    "\n",
    "\n",
    "# Initializei the model with the simclr model weights.\n",
    "init_model_from_consolidated_weights(\n",
    "    config=cfg,\n",
    "    model=model,\n",
    "    state_dict=weights,\n",
    "    state_dict_key_name=\"classy_state_dict\",\n",
    "    skip_layers=[],  # Use this if you do not want to load all layers\n",
    ")\n",
    "\n",
    "print(\"Weights have loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9120f9",
   "metadata": {},
   "source": [
    "#### Extra info\n",
    "- VISSL uses the ResNeXT50 class, which is their custom wrapper class\n",
    "    - ResNeXT50 wrapper class is defined at https://github.com/facebookresearch/vissl/blob/04788de934b39278326331f7a4396e03e85f6e55/vissl/models/trunks/resnext.py\n",
    "    - ResNet base class https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py for interface of the __init__ method.\n",
    "    - the model of this wrapper class is a torchvision.models.ResNet() which we will reconstruct here based on the YAML config parameters.\n",
    "- checkpoints from pretraining are stored on /home/olivier/Documents/master/mp/checkpoints/sku110k/\n",
    "    - checkpoints have phase numbers: in VISSL, if the workflow involves training and testing both, the number of phases = train phases + test epochs. So if we alternate train and test, the phase number is: 0 (train), 1 (test), 2 (train), 3 (test)... and train_phase_idx is always: 0 (corresponds to phase0), 1 (correponds to phase 2)\n",
    "    - The weights are stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7238a7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vissl checkpoint\n",
      "Checkpoint contains:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olivier/Documents/master/mp/.venv/lib/python3.8/site-packages/torch/cuda/__init__.py:104: UserWarning: \n",
      "NVIDIA GeForce RTX 4070 Laptop GPU with CUDA capability sm_89 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 4070 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>phase_idx</th>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_num</th>\n",
       "      <td>1880199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_phase_idx</th>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration</th>\n",
       "      <td>1880100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>consolidated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Value\n",
       "phase_idx                  99\n",
       "iteration_num         1880199\n",
       "train_phase_idx            99\n",
       "iteration             1880100\n",
       "type             consolidated"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint also contains elements loss and classy_state_dict\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading vissl checkpoint\")\n",
    "ssl_checkpoint = torch.load(Path(weights_file))\n",
    "print(\"Checkpoint contains:\")\n",
    "dataframe_dict = dict()\n",
    "dataframe_dict[\"phase_idx\"] = ssl_checkpoint[\"phase_idx\"]\n",
    "dataframe_dict[\"iteration_num\"] = ssl_checkpoint[\"iteration_num\"]\n",
    "dataframe_dict[\"train_phase_idx\"] = ssl_checkpoint[\"train_phase_idx\"]\n",
    "dataframe_dict[\"iteration\"] = ssl_checkpoint[\"iteration\"]\n",
    "dataframe_dict[\"type\"] = ssl_checkpoint[\"type\"]\n",
    "df = pd.DataFrame(data=dataframe_dict.values(), index=dataframe_dict.keys(),columns=[\"Value\"])\n",
    "display(df)\n",
    "if(\"loss\", \"classy_state_dict\" in ssl_checkpoint.keys()):\n",
    "    print(\"Checkpoint also contains elements loss and classy_state_dict\")\n",
    "\n",
    "#the weights of the trunk resnet network are stored in a nested dict:    \n",
    "#print(ssl_checkpoint[\"classy_state_dict\"][\"base_model\"][\"model\"][\"trunk\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2dc577",
   "metadata": {},
   "source": [
    "## Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a399ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def extract_features(path):\n",
    "    image = Image.open(path)\n",
    "    # Convert images to RGB. This is important\n",
    "    # as the model was trained on RGB images.\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "    # Image transformation pipeline.\n",
    "    pipeline = transforms.Compose([\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "    ])\n",
    "    x = pipeline(image)\n",
    "\n",
    "    #unsqueeze adds a dim for batch size (with 1 element the entire input tensor of the image)\n",
    "    features = model(x.unsqueeze(0))\n",
    "\n",
    "    features_shape = features[0].shape\n",
    "    #print(f\"Features extracted have the shape: { features_shape }\")\n",
    "    return features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf7072b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2048])\n",
      "['CawstonDry', 'CawstonDry', 'CawstonDry', 'MinuteMaidAppelPerzik', 'CarrefourSmoothieAardbeiBlauweBessen', 'CarrefourSmoothieAardbeiBlauweBessen', 'CarrefourSmoothieAardbeiBlauweBessen', 'CarrefourSmoothieAardbeiBlauweBessen', 'GiniZeroFles1,5L', 'GiniZeroFles1,5L']\n"
     ]
    }
   ],
   "source": [
    "CornerShop = Path(\"/home/olivier/Documents/master/mp/CornerShop/CornerShop/crops\")\n",
    "\n",
    "#create an iterator over all jpg files in cornershop map and put elements in a list\n",
    "img_paths = list(CornerShop.glob(\"*/*.jpg\")) #**/*.jpg to look into all subdirs for jpgs and iterate over them\n",
    "#extract the corresponding labels (folder names)\n",
    "labels = [p.parent.stem for p,_ in zip(img_paths,range(20)) ] #stem attr, conatins foldername \n",
    "#path.stem=filename without extension\n",
    "#path.name=filename with extension\n",
    "fts_stack = torch.stack([extract_features(p).squeeze() for p,_ in zip(img_paths,range(20)) ])\n",
    "print(fts_stack.shape)\n",
    "print(labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f9c299",
   "metadata": {},
   "source": [
    "results of the feature extraction:\n",
    "- fts_stack: contains n rows and 2048 columns (features), this is a stack of features from multiple query images\n",
    "- labels: list with the corresponding labels of the feature stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea772812",
   "metadata": {},
   "source": [
    "## Comparing features\n",
    "Here we will investigate relations between the features from different images with:\n",
    "- Inner product\n",
    "- Cosine simularity\n",
    "- Euclidian distance\n",
    "- Euclidian distance (normalized features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e575080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data to save statistics about the feature comparisons\n",
    "data = np.zeros((4,4),dtype=float)\n",
    "data_index=[\"max\",\"min\",\"avg\",\"std_dev\"] #labels for dataframe\n",
    "data_columns=np.empty((4),dtype=object) #numpy array of strings (objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a61c4",
   "metadata": {},
   "source": [
    "### Inner product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f8de8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three examples from inner_product tensor:\n",
      "tensor([[1.3904, 1.2524, 1.2929, 1.3192, 1.2936, 1.2811, 1.2357, 1.2880, 1.2943,\n",
      "         1.2757, 1.3023, 1.3063, 1.2646, 1.3191, 1.2566, 1.2995, 1.2536, 1.3049,\n",
      "         1.2246, 1.3002],\n",
      "        [1.2524, 1.3009, 1.2350, 1.2795, 1.2477, 1.2393, 1.1961, 1.2399, 1.2564,\n",
      "         1.2308, 1.2538, 1.2570, 1.2235, 1.2785, 1.2219, 1.2604, 1.2163, 1.2590,\n",
      "         1.1932, 1.2535],\n",
      "        [1.2929, 1.2350, 1.3375, 1.2941, 1.2702, 1.2618, 1.2182, 1.2656, 1.2738,\n",
      "         1.2506, 1.2807, 1.2823, 1.2465, 1.3012, 1.2449, 1.2836, 1.2342, 1.2807,\n",
      "         1.2075, 1.2773]])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inner_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.465119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.180677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>1.290121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_dev</th>\n",
       "      <td>0.044062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         inner_product\n",
       "max           1.465119\n",
       "min           1.180677\n",
       "avg           1.290121\n",
       "std_dev       0.044062"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#multiply the features of one tensor with all other tensors\n",
    "inner_product = fts_stack.matmul(fts_stack.T)\n",
    "print(\"Three examples from inner_product tensor:\\n{}\".format(inner_product[0:3]))\n",
    "\n",
    "#save statistics data\n",
    "data_columns[0] = \"inner_product\"\n",
    "data[0][0]= inner_product.max()\n",
    "data[1][0]= inner_product.min()\n",
    "data[2][0]= torch.mean(inner_product)\n",
    "data[3][0]= torch.std(inner_product)\n",
    "\n",
    "#display statistics of inner product\n",
    "df = pd.DataFrame(data=data[:,0],index=data_index, columns=[data_columns[0]])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca8ac4",
   "metadata": {},
   "source": [
    "### Cosine simularity\n",
    "Here we normalize the features and then calculate the inner product with all other tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ee6222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZE features in feature stack:\n",
    "fts_stack_norm = fts_stack / fts_stack.norm(dim=1,keepdim=True) \n",
    "#fts.norm(dim=1,keepdim=True)\n",
    "# dim=1: calculate norm over the second dimension (features/columns)\n",
    "# keepdim=True: keep batch/stack dimension of features\n",
    "\n",
    "#.norm is deprecated, newer version https://pytorch.org/docs/stable/generated/torch.linalg.matrix_norm.html#torch.linalg.matrix_norm \n",
    "#fts_stack_norm = fts_stack / torch.linalg.matrix_norm(fts_stack, dim=1, keepdim=True) #newer version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f83a914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three examples from cosim tensor:\n",
      "tensor([[1.0000, 0.9312, 0.9480, 0.9310, 0.9250, 0.9285, 0.9254, 0.9270, 0.9295,\n",
      "         0.9246, 0.9295, 0.9253, 0.9227, 0.9242, 0.9247, 0.9235, 0.9154, 0.9311,\n",
      "         0.9180, 0.9259],\n",
      "        [0.9312, 1.0000, 0.9362, 0.9335, 0.9224, 0.9286, 0.9261, 0.9226, 0.9328,\n",
      "         0.9223, 0.9252, 0.9205, 0.9229, 0.9260, 0.9296, 0.9261, 0.9182, 0.9288,\n",
      "         0.9247, 0.9229],\n",
      "        [0.9480, 0.9362, 1.0000, 0.9312, 0.9261, 0.9325, 0.9302, 0.9288, 0.9327,\n",
      "         0.9242, 0.9321, 0.9262, 0.9273, 0.9295, 0.9341, 0.9301, 0.9189, 0.9318,\n",
      "         0.9229, 0.9274]])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cosim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.915432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>0.936654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_dev</th>\n",
       "      <td>0.016745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cosim\n",
       "max      1.000001\n",
       "min      0.915432\n",
       "avg      0.936654\n",
       "std_dev  0.016745"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#calculate cosine simularity (cosim)\n",
    "#fts_stack is a matrix with n rows and 2048 columns (features)\n",
    "#matrix product of fts_stack * fts_stack^T = cosin_sim with all other images from the stack\n",
    "cosim = fts_stack_norm.matmul(fts_stack_norm.T)\n",
    "print(\"Three examples from cosim tensor:\\n{}\".format(cosim[0:3]))\n",
    "\n",
    "#save statistics data\n",
    "data_columns[1] = \"cosim\" \n",
    "data[0][1]= cosim.max()\n",
    "data[1][1]= cosim.min()\n",
    "data[2][1]= torch.mean(cosim)\n",
    "data[3][1]= torch.std(cosim)\n",
    "\n",
    "#display statistics of cosim\n",
    "df = pd.DataFrame(data=data[:,1],index=data_index, columns=[data_columns[1]])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964761b3",
   "metadata": {},
   "source": [
    "### Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecd061cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three examples from eucl_dist tensor:\n",
      "tensor([[0.0000, 0.4319, 0.3771, 0.4429, 0.4582, 0.4442, 0.4489, 0.4505, 0.4431,\n",
      "         0.4561, 0.4446, 0.4595, 0.4606, 0.4662, 0.4531, 0.4642, 0.4816, 0.4394,\n",
      "         0.4702, 0.4562],\n",
      "        [0.4319, 0.0000, 0.4105, 0.4313, 0.4605, 0.4374, 0.4371, 0.4576, 0.4274,\n",
      "         0.4565, 0.4529, 0.4693, 0.4527, 0.4572, 0.4304, 0.4516, 0.4659, 0.4420,\n",
      "         0.4409, 0.4604],\n",
      "        [0.3771, 0.4105, 0.0000, 0.4397, 0.4513, 0.4277, 0.4283, 0.4412, 0.4295,\n",
      "         0.4530, 0.4333, 0.4540, 0.4421, 0.4475, 0.4193, 0.4407, 0.4668, 0.4343,\n",
      "         0.4500, 0.4483]])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eucl_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.481599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>0.407248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_dev</th>\n",
       "      <td>0.097231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         eucl_dist\n",
       "max       0.481599\n",
       "min       0.000000\n",
       "avg       0.407248\n",
       "std_dev   0.097231"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eucl_dist = [] \n",
    "for tensor in fts_stack:\n",
    "    d = [] #store all distances from this tensor to all the other tensors\n",
    "    for other_tensor in fts_stack:\n",
    "        d_to = (tensor - other_tensor).pow(2).sum().sqrt() #d(tensor, other_tensor)=euclid distance\n",
    "        d.append(d_to)\n",
    "    d = torch.tensor(d)\n",
    "    #print(\"distance tensor has shape {}\".format(d.shape))\n",
    "    #add tensor to euclidian distances \n",
    "    eucl_dist.append(d)\n",
    "eucl_dist = torch.stack(eucl_dist)\n",
    "#print(\"eucl_dist has shape {}\".format(eucl_dist.shape))\n",
    "print(\"Three examples from eucl_dist tensor:\\n{}\".format(eucl_dist[0:3]))\n",
    "\n",
    "#save statistics data\n",
    "data_columns[2] = \"eucl_dist\" \n",
    "data[0][2]= eucl_dist.max()\n",
    "data[1][2]= eucl_dist.min()\n",
    "data[2][2]= torch.mean(eucl_dist)\n",
    "data[3][2]= torch.std(eucl_dist)\n",
    "\n",
    "#display statistics of euclidian distance\n",
    "df = pd.DataFrame(data=data[:,2],index=data_index, columns=[data_columns[2]])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfcf7c4",
   "metadata": {},
   "source": [
    "### Euclidian distance (normalized features)\n",
    "Using normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17bda747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three examples from eucl_dist tensor:\n",
      "tensor([[0.0000, 0.3709, 0.3223, 0.3716, 0.3874, 0.3781, 0.3864, 0.3822, 0.3755,\n",
      "         0.3883, 0.3755, 0.3864, 0.3932, 0.3893, 0.3880, 0.3911, 0.4113, 0.3711,\n",
      "         0.4050, 0.3849],\n",
      "        [0.3709, 0.0000, 0.3571, 0.3647, 0.3940, 0.3778, 0.3845, 0.3934, 0.3667,\n",
      "         0.3943, 0.3869, 0.3987, 0.3927, 0.3846, 0.3753, 0.3844, 0.4044, 0.3774,\n",
      "         0.3880, 0.3927],\n",
      "        [0.3223, 0.3571, 0.0000, 0.3710, 0.3845, 0.3675, 0.3737, 0.3775, 0.3669,\n",
      "         0.3892, 0.3686, 0.3843, 0.3813, 0.3755, 0.3631, 0.3739, 0.4027, 0.3694,\n",
      "         0.3928, 0.3809]])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eucl_dist_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.411261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>0.346172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_dev</th>\n",
       "      <td>0.082906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         eucl_dist_norm\n",
       "max            0.411261\n",
       "min            0.000000\n",
       "avg            0.346172\n",
       "std_dev        0.082906"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eucl_dist_norm = [] \n",
    "for tensor in fts_stack_norm:\n",
    "    d = [] #store all distances from this tensor to all the other tensors\n",
    "    for other_tensor in fts_stack_norm:\n",
    "        d_to = (tensor - other_tensor).pow(2).sum().sqrt() #d(tensor, other_tensor)=euclid distance\n",
    "        d.append(d_to)\n",
    "    d = torch.tensor(d)\n",
    "    #print(\"distance tensor has shape {}\".format(d.shape))\n",
    "    #add tensor to euclidian distances \n",
    "    eucl_dist_norm.append(d)\n",
    "eucl_dist_norm = torch.stack(eucl_dist_norm)\n",
    "#print(\"eucl_dist has shape {}\".format(eucl_dist.shape))\n",
    "print(\"Three examples from eucl_dist tensor:\\n{}\".format(eucl_dist_norm[0:3]))\n",
    "\n",
    "#save statistics data\n",
    "data_columns[3] = \"eucl_dist_norm\"\n",
    "data[0][3]= eucl_dist_norm.max()\n",
    "data[1][3]= eucl_dist_norm.min()\n",
    "data[2][3]= torch.mean(eucl_dist_norm)\n",
    "data[3][3]= torch.std(eucl_dist_norm)\n",
    "\n",
    "#display statistics of euclidian distance normalized\n",
    "df = pd.DataFrame(data=data[:,3],index=data_index, columns=[data_columns[3]])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a82b9dd",
   "metadata": {},
   "source": [
    "Let's compare the statistics of these 4 methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cf8ca84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inner_product</th>\n",
       "      <th>cosim</th>\n",
       "      <th>eucl_dist</th>\n",
       "      <th>eucl_dist_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.465119</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>0.481599</td>\n",
       "      <td>0.411261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.180677</td>\n",
       "      <td>0.915432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>1.290121</td>\n",
       "      <td>0.936654</td>\n",
       "      <td>0.407248</td>\n",
       "      <td>0.346172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_dev</th>\n",
       "      <td>0.044062</td>\n",
       "      <td>0.016745</td>\n",
       "      <td>0.097231</td>\n",
       "      <td>0.082906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         inner_product     cosim  eucl_dist  eucl_dist_norm\n",
       "max           1.465119  1.000001   0.481599        0.411261\n",
       "min           1.180677  0.915432   0.000000        0.000000\n",
       "avg           1.290121  0.936654   0.407248        0.346172\n",
       "std_dev       0.044062  0.016745   0.097231        0.082906"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=data,index=data_index, columns=data_columns)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
