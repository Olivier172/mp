run in virtual env
>> source .venv/bin/activate

trainingen uitvoeren
>> cd vissl/
>> python tools/run_distributed_engines.py ...

dataset moet in de catalogus staan, dus in /home/olivier/Documents/mp/vissl/configs/config/dataset_catalog.json
entry toevoegen: (dan kan je daarna in yaml files gewoon de naam "sku110k_folder" gebruiken)
{
    "sku110k_folder": {
        "train": ["/home/olivier/Documents/mp/cropped_images/train", "<unused>"],
        "val": ["/home/olivier/Documents/mp/cropped_images/test", "<unused>"]
    },
    ....
}
zorg dat de dataset structuur van het volgende formaat is:
    # cropped_images
    #     ├── test
    #     │   └── unlabeled
    #     ├── train
    #     │   └── unlabeled
    #     └── val
    #         └── unlabeled

Maak de folder waar de training de resultaten naar toe schrijft zeker aan
bv in mp/checkpoints/rotnet_full 
en specifier het pad naar deze folder in je yaml file bij CHECKPOINT.DIR

dan een eigen config maken om bepaalde keys van de training hyperparameters te overschrijven
bv in /home/olivier/Documents/mp/vissl/configs/config/pretrain/rotnet maak een folder dataset met daarin een file sku110k.yaml
Deze ziet er bv zo uit:
# @package _global_
config:
  VERBOSE: True
  DATA:
    NUM_DATALOADER_WORKERS: 2 #1 cpu
    TRAIN:
      DATA_SOURCES: [disk_folder]
      DATASET_NAMES: [sku110k_folder]
    TEST:
      DATA_SOURCES: [disk_folder]
      DATASET_NAMES: [sku110k_folder]
  HOOKS:
    TENSORBOARD_SETUP:
      USE_TENSORBOARD: True # whether to use tensorboard for the visualization
      LOG_DIR: "/home/olivier/Documents/mp/checkpoints/tensorboard" # log directory for tensorboard events
      EXPERIMENT_LOG_DIR: "sku110k_rotnet"
      FLUSH_EVERY_N_MIN: 5  # flush logs every n minutes
      LOG_PARAMS: True # whether to log the model parameters to tensorboard
      LOG_PARAMS_GRADIENTS: True # whether to log the model parameters gradients to tensorboard 
      LOG_PARAMS_EVERY_N_ITERS: -1 #log params every epoch
  DISTRIBUTED:
    NUM_NODES: 1 #1 gpu 
    NUM_PROC_PER_NODE: 1 #1 gpu
  CHECKPOINT:
    DIR: "/home/olivier/Documents/mp/checkpoints/rotnet_full"
    CHECKPOINT_FREQUENCY: 25

en voeg dan aan het trainings commando +config/pretrain/rotnet/dataset=sku110k toe

Gouden refentie voor de YAML file opties:
https://github.com/facebookresearch/vissl/blob/main/vissl/config/defaults.yaml 

checkpoint frequency instellen:
# how frequently should the model be checkpointed. The model is checkpointed
# only if the training is on (i.e. the eval phases are never checkpointed).
# epochs start from 0 so the 1st epoch is always checkpointed.
# Examples:
#   CHECKPOINT_FREQUENCY = 1 -> checkpoint after every training epoch
#   CHECKPOINT_FREQUENCY = N -> checkpoint after every N training epochs
#                               when train_epoch_num % CHECKPOINT_FREQ = 0.
# In VISSL, if the workflow involves training and testing both, the number of
# phases = train phases + test epochs. So if we alternate train and test, the
# phase number is: 0 (train), 1 (test), 2 (train), 3 (test)...
# and train_phase_idx is always: 0 (corresponds to phase0), 1 (correponds to phase 2)
# For deciding whether to checkpointing, we
# always count the number of training phases train_phase_idx and checkpoint. However,
# the checkpoint file has number phase_idx.
CHECKPOINT_FREQUENCY: 1

TRAININGSCOMMANDOS:

rotnet:
python tools/run_distributed_engines.py config=pretrain/rotnet/rotnet_8gpu_resnet \
+config/pretrain/rotnet/dataset=sku110k

jigsaw:
python tools/run_distributed_engines.py config=pretrain/jigsaw/jigsaw_8gpu_resnet \
+config/pretrain/jigsaw/dataset=sku110k

moco:
python tools/run_distributed_engines.py config=pretrain/moco/moco_1node_resnet \
+config/pretrain/moco/dataset=sku110k

simclr:
python tools/run_distributed_engines.py config=pretrain/simclr/simclr_8node_resnet \
+config/pretrain/simclr/dataset=sku110k

swav:
python tools/run_distributed_engines.py config=pretrain/swav/swav_8node_resnet \
+config/pretrain/swav/dataset=sku110k
